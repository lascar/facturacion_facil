#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script pour exÃ©cuter les tests de FacturaciÃ³n FÃ¡cil
"""

import sys
import os
import subprocess

def run_tests():
    """ExÃ©cute tous les tests avec pytest"""

    print("ğŸ§ª Ejecutando tests de FacturaciÃ³n FÃ¡cil...")
    print("=" * 50)

    # DÃ©terminer la commande Python Ã  utiliser
    python_cmd = sys.executable

    # Commandes de test
    commands = [
        # Tests basiques avec couverture
        [python_cmd, "-m", "pytest", "-v", "--cov=.", "--cov-report=term-missing"],

        # GÃ©nÃ©rer rapport HTML de couverture
        [python_cmd, "-m", "pytest", "--cov=.", "--cov-report=html"],
    ]
    
    for i, cmd in enumerate(commands, 1):
        print(f"\nğŸ“‹ Ã‰tape {i}/{len(commands)}: {' '.join(cmd)}")
        print("-" * 30)
        
        try:
            result = subprocess.run(cmd, check=True, capture_output=False)
            print(f"âœ… Ã‰tape {i} terminÃ©e avec succÃ¨s")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Erreur dans l'Ã©tape {i}: {e}")
            return False
        except FileNotFoundError:
            print(f"âŒ Commande non trouvÃ©e: {cmd[0]}")
            print("ğŸ’¡ Installez pytest avec: pip install -r requirements-dev.txt")
            return False
    
    print("\nğŸ‰ Tous les tests sont terminÃ©s!")
    print("ğŸ“Š Rapport de couverture HTML gÃ©nÃ©rÃ© dans: htmlcov/index.html")
    return True

def run_specific_tests():
    """ExÃ©cute des tests spÃ©cifiques selon les arguments"""
    
    if len(sys.argv) < 2:
        return run_tests()
    
    test_type = sys.argv[1].lower()
    
    python_cmd = sys.executable

    test_commands = {
        "unit": [python_cmd, "-m", "pytest", "tests/test_database/", "-v"],
        "ui": [python_cmd, "-m", "pytest", "tests/test_ui/", "-v"],
        "utils": [python_cmd, "-m", "pytest", "tests/test_utils/", "-v"],
        "advanced": [python_cmd, "-m", "pytest", "tests/test_advanced/", "-v"],
        "parametrized": [python_cmd, "-m", "pytest", "tests/test_advanced/test_parametrized.py", "-v"],
        "property": [python_cmd, "-m", "pytest", "tests/test_advanced/test_property_based.py", "-v"],
        "performance": [python_cmd, "-m", "pytest", "tests/test_advanced/test_performance.py", "-v"],
        "integration": [python_cmd, "-m", "pytest", "tests/test_advanced/test_integration.py", "-v"],
        "security": [python_cmd, "-m", "pytest", "tests/test_advanced/test_security.py", "-v"],
        "benchmark": [python_cmd, "-m", "pytest", "--benchmark-only", "-v"],
        "fast": [python_cmd, "-m", "pytest", "-v", "-m", "not slow"],
        "slow": [python_cmd, "-m", "pytest", "-v", "-m", "slow"],
        "parallel": [python_cmd, "-m", "pytest", "-n", "auto", "-v"],
        "coverage": [python_cmd, "-m", "pytest", "--cov=.", "--cov-report=html", "--cov-report=term"],
        "lint": [python_cmd, "-m", "flake8", "."],
        "format": [python_cmd, "-m", "black", ".", "--check"],
    }
    
    if test_type in test_commands:
        cmd = test_commands[test_type]
        print(f"ğŸ§ª Ejecutando: {' '.join(cmd)}")
        
        try:
            subprocess.run(cmd, check=True)
            print(f"âœ… {test_type} tests completados")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ Error en {test_type} tests: {e}")
            return False
        except FileNotFoundError:
            print(f"âŒ Comando no encontrado: {cmd[0]}")
            return False
    else:
        print(f"âŒ Tipo de test desconocido: {test_type}")
        print("ğŸ“‹ Tipos disponibles:")
        print("  BÃ¡sicos: unit, ui, utils, advanced")
        print("  Avanzados: parametrized, property, performance, integration, security")
        print("  Especiales: benchmark, fast, slow, parallel, coverage")
        print("  Calidad: lint, format")
        return False

def show_help():
    """Affiche l'aide"""
    help_text = """
ğŸ§ª Script de Tests - FacturaciÃ³n FÃ¡cil

Uso:
    python run_tests.py [tipo]

Tipos de tests disponibles:
    (sin argumentos)  - Ejecuta todos los tests con cobertura

    Tests bÃ¡sicos:
    unit             - Tests de base de datos y modelos
    ui               - Tests de interfaz de usuario
    utils            - Tests de utilidades
    advanced         - Todos los tests avanzados

    Tests avanzados:
    parametrized     - Tests parametrizados con mÃºltiples casos
    property         - Tests basados en propiedades (Hypothesis)
    performance      - Tests de rendimiento y benchmarks
    integration      - Tests de integraciÃ³n de workflows
    security         - Tests de seguridad y validaciÃ³n

    Tests especiales:
    benchmark        - Solo benchmarks de rendimiento
    fast             - Tests rÃ¡pidos (excluye 'slow')
    slow             - Solo tests lentos
    parallel         - Tests en paralelo (requiere pytest-xdist)
    coverage         - Tests con reporte de cobertura detallado

    Calidad de cÃ³digo:
    lint             - VerificaciÃ³n de estilo con flake8
    format           - VerificaciÃ³n de formato con black

Ejemplos:
    python run_tests.py                    # Todos los tests
    python run_tests.py unit               # Tests unitaires
    python run_tests.py advanced           # Tests avanzados
    python run_tests.py performance        # Tests de rendimiento
    python run_tests.py security           # Tests de seguridad
    python run_tests.py benchmark          # Solo benchmarks
    python run_tests.py parallel           # Tests en paralelo
    python run_tests.py coverage           # Con cobertura
    python run_tests.py lint               # VerificaciÃ³n estilo

InstalaciÃ³n de dependencias:
    pip install -r requirements-dev.txt
"""
    print(help_text)

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] in ["-h", "--help", "help"]:
        show_help()
    else:
        success = run_specific_tests()
        sys.exit(0 if success else 1)
